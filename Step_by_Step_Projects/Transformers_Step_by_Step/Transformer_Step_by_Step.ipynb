{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbzZLqIPv6b7",
    "outputId": "19f2fc2b-6f1d-4b43-fd50-4c513e3936fd"
   },
   "source": [
    "# Transformer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OpwqWL2QH5G"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Positional Encoding\n",
    "\n",
    "In sequence to sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model.  However, when you train a Transformer network using multi-head attention, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful - you can specifically encode the positions of your inputs and pass them into the network using these sine and cosine formulas:\n",
    "    \n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{1}$$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{2}$$\n",
    "\n",
    "* $d$ is the dimension of the word embedding and positional encoding\n",
    "* $pos$ is the position of the word.\n",
    "* $k$ refers to each of the different dimensions in the positional encodings, with $i$ equal to $k$ $//$ $2$.\n",
    "\n",
    "To develop some intuition about positional encodings, we can think of them broadly as a feature that contains the information about the relative positions of words. The sum of the positional encoding and word embedding is ultimately what is fed into the model. If we just hard code the positions by adding a matrix of 1's or whole numbers to the word embedding (for example), the semantic meaning is distorted. Conversely, the values of the sine and cosine equations are small enough (between -1 and 1) that when we add the positional encoding to a word embedding, the word embedding is not significantly distorted, and is instead enriched with positional information. Using a combination of these two equations helps the Transformer network attend to the relative positions of the input data.\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Sine and Cosine Angles\n",
    "\n",
    "Notice that even though the sine and cosine positional encoding equations take in different arguments (`2i` versus `2i+1`, or even versus odd numbers) the inner terms for both equations are the same: $$\\theta(pos, i, d) = \\frac{pos}{10000^{\\frac{2i}{d}}} \\tag{3}$$\n",
    "\n",
    "Consider the inner term as you calculate the positional encoding for a word in a sequence.<br> \n",
    "\n",
    "$PE_{(pos, 0)}= sin\\left(\\frac{pos}{{10000}^{\\frac{0}{d}}}\\right)$, since solving `2i = 0` gives `i = 0`\n",
    "<br>\n",
    "\n",
    "$PE_{(pos, 1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{0}{d}}}\\right)$, since solving `2i + 1 = 1` gives `i = 0`\n",
    "\n",
    "The angle is the same for both! The angles for $PE_{(pos, 2)}$ and $PE_{(pos, 3)}$ are the same as well, since for both, `i = 1` and therefore the inner term is $\\left(\\frac{pos}{{10000}^{\\frac{2}{d}}}\\right)$. This relationship holds true for all paired sine and cosine curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPzwMVfcQpT-"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, k, d):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "    \n",
    "    Arguments:\n",
    "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "        k --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
    "        d(integer) -- Encoding size\n",
    "    \n",
    "    Returns:\n",
    "        angles -- (pos, d) numpy array \n",
    "    \"\"\"\n",
    "    \n",
    "    i = k//2\n",
    "    angles = pos/(10000**(2*i/d)) # (sequence_length, encoding_size)\n",
    "    \n",
    "    return angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Sine and Cosine Positional Encodings\n",
    "\n",
    "Now you can use the angles you computed to calculate the sine and cosine positional encodings.\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y78txxoHQtwG"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding -- (1, positions, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "\n",
    "    angle_rads = get_angles(np.reshape(np.arange(positions), (positions, 1)), np.reshape(np.arange(d), (1, d)), d)\n",
    "  \n",
    "    # applying sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # applying cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...] # (1, sequence_length, encoding_size)\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Masking\n",
    "\n",
    "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence. \n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Padding Mask\n",
    "\n",
    "Oftentimes your input sequence will exceed the maximum length of a sequence your network can process. Let's say the maximum length of your model is five, it is fed the following sequences:\n",
    "\n",
    "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"], \n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
    "     [\"Exciting\", \"!\"]]\n",
    "\n",
    "which might get vectorized as:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600]]\n",
    "    \n",
    "When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99],\n",
    "     [ 2344, 345, 1284, 15, 0],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600, 0, 0, 0]]\n",
    "    \n",
    "Sequences longer than the maximum length of five will be truncated, and zeros will be added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, zeros will also be added for padding. However, these zeros will affect the softmax calculation - this is when a padding mask comes in handy! You will need to define a boolean mask that specifies to which elements you must attend(1) and which elements you must ignore(0). Later you will use that mask to set all the zeros in the sequence to a value close to negative infinity (-1e9).\n",
    "\n",
    "**Note:** The below function only creates the mask of an _already padded sequence_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOL9XWsFQxxo"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        decoder_token_ids -- (n, m) matrix\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (n, 1, m) binary tensor\n",
    "    \"\"\"    \n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "  \n",
    "    # adding extra dimensions to add the padding to the attention logits. \n",
    "    # this will allow for broadcasting later when comparing sequences\n",
    "    \n",
    "    return seq[:, tf.newaxis, :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Look-ahead Mask\n",
    "\n",
    "The look-ahead mask follows similar intuition. In training, you will have access to the complete correct output of your training example. The look-ahead mask helps your model pretend that it correctly predicted a part of the output and see if, *without looking ahead*, it can correctly predict the next output. \n",
    "\n",
    "For example, if the expected correct output is `[1, 2, 3]` and you wanted to see if given that the model correctly predicted the first value it could predict the second value, you would mask out the second and third values. So you would input the masked sequence `[1, -1e9, -1e9]` and see if it could generate `[1, 2, -1e9]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9O9UbM31Q3hK"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Returns a lower triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        sequence_length -- matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (size, size) tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
    "    \n",
    "    return mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG0gPyv0oDBi"
   },
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Self-Attention\n",
    "\n",
    "As the authors of the Transformers paper state, \"Attention is All You Need\". \n",
    "\n",
    "<center><img src=\"images/self-attention.png\" alt=\"Encoder\" width=\"50%\" heigth=\"50%\"></center>\n",
    "<caption><center><b>Figure 1:</b> Self-Attention calculation visualization</center></caption>\n",
    "<br>\n",
    "    \n",
    "The use of self-attention paired with traditional convolutional networks allows for parallelization, which speeds up training. You will implement **scaled dot product attention**, which takes in a query, key, value, and a mask as inputs to return rich attention-based vector representations of the words in your sequence. This type of self-attention can be mathematically expressed as:\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask you choose to apply \n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSysk_rjQ7lp"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type (padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q -- query shape == (..., seq_len_q, depth)\n",
    "        k -- key shape == (..., seq_len_k, depth)\n",
    "        v -- value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scaling matmul_qk\n",
    "    dk = k.shape[1]\n",
    "    scaled_attention_logits = matmul_qk/dk**(1/2)\n",
    "\n",
    "    # adding the mask to the scaled tensor\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (1. - mask)*-1.0e9\n",
    "\n",
    "    # applying softmax to the last axis    \n",
    "    attention_weights = tf.keras.activations.softmax(scaled_attention_logits) # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # calculating v\n",
    "    output = tf.matmul(attention_weights, v) # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blS0pEpTqRVI"
   },
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Encoder\n",
    "\n",
    "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder, which you'll build later in the assignment. In this section of the assignment, you will implement the Encoder by pairing multi-head attention and a feed forward neural network (Figure 2a). \n",
    "\n",
    "\n",
    "<center><img src=\"images/encoder_layer.png\" alt=\"Encoder\" width=\"25%\"></center>\n",
    "<caption><center><b>Figure 2a:</b> Transformer encoder layer</center></caption>\n",
    "\n",
    "* `MultiHeadAttention` you can think of as computing the self-attention several times to detect different features. \n",
    "* Feed forward neural network contains two Dense layers which we'll implement as the function `FullyConnected`\n",
    "\n",
    "Your input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC5vJhz29vZR"
   },
   "outputs": [],
   "source": [
    "def FullyConnected(fully_connected_dim, embedding_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R65WbX5wqYYH"
   },
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 - Encoder Layer\n",
    "\n",
    "Now you can pair multi-head attention and feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization to help speed up training (Figure 2a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIufbrc-9_2u"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This architecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 num_heads, \n",
    "                 fully_connected_dim,\n",
    "                 dropout_rate=0.1, \n",
    "                 layernorm_eps=1e-6):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads = num_heads,\n",
    "                                      key_dim = embedding_dim,\n",
    "                                      dropout = dropout_rate)\n",
    "\n",
    "        self.ffn = FullyConnected(fully_connected_dim = fully_connected_dim,\n",
    "                                  embedding_dim = embedding_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon = layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon = layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not treated as part of the input\n",
    "                    \n",
    "        Returns:\n",
    "            encoder_layer_out -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculating self-attention using mha\n",
    "        self_mha_output = self.mha(x, x, attention_mask=mask, training=training)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # applying skip conection and layer normalization \n",
    "        skip_x_attention = self.layernorm1(self_mha_output + x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # passing the output of the mha through a ffn\n",
    "        ffn_output = self.ffn(skip_x_attention)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # applying dropout to ffn output during training\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training) # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # applying skip conection and layer normalization\n",
    "        encoder_layer_out = self.layernorm2(ffn_output + skip_x_attention)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        return encoder_layer_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Full Encoder\n",
    " \n",
    "\n",
    "<center><img src=\"images/encoder.png\" alt=\"Encoder\" width=\"30%\"></center>\n",
    "<caption><center><b>Figure 2b:</b> Transformer Encoder</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j2Tjr0K0t0I"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "    \"\"\"  \n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_layers, \n",
    "                 embedding_dim, \n",
    "                 num_heads, \n",
    "                 fully_connected_dim, \n",
    "                 input_vocab_size,\n",
    "                 maximum_position_encoding, \n",
    "                 dropout_rate=0.1, \n",
    "                 layernorm_eps=1e-6):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim = self.embedding_dim,\n",
    "                                        num_heads = num_heads,\n",
    "                                        fully_connected_dim = fully_connected_dim,\n",
    "                                        dropout_rate = dropout_rate,\n",
    "                                        layernorm_eps = layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not treated as part of the input\n",
    "                    \n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # passing input through the Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # scaleing embedding\n",
    "        x = tf.cast(x, tf.float32)*self.embedding_dim**(1/2)\n",
    "        \n",
    "        # adding the position encoding\n",
    "        x += self.pos_encoding[:,:seq_len,:]\n",
    "        \n",
    "        # passing the encoded embedding through a dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # passing the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Decoder\n",
    "\n",
    "The Decoder layer takes the K and V matrices generated by the Encoder and computes the second multi-head attention layer with the Q matrix from the output (Figure 3a).\n",
    "\n",
    "<center><img src=\"images/decoder_layer.png\" alt=\"Encoder\" width=\"20%\"/></center>\n",
    "<caption><center><b>Figure 3a:</b> Transformer Decoder layer</center></caption>\n",
    "\n",
    "<a name='5-1'></a>    \n",
    "### 5.1 - Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEouNFvCzMeT"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks, \n",
    "    one that takes the new input and uses self-attention, and the other \n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 num_heads, \n",
    "                 fully_connected_dim, \n",
    "                 dropout_rate=0.1, \n",
    "                 layernorm_eps=1e-6):\n",
    "        \n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(num_heads = num_heads,\n",
    "                                       key_dim = embedding_dim,\n",
    "                                       dropout = dropout_rate)\n",
    "\n",
    "        self.mha2 = MultiHeadAttention(num_heads = num_heads,\n",
    "                                       key_dim = embedding_dim,\n",
    "                                       dropout = dropout_rate)\n",
    "\n",
    "        self.ffn = FullyConnected(fully_connected_dim = fully_connected_dim,\n",
    "                                  embedding_dim = embedding_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon = layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon = layernorm_eps)\n",
    "        self.layernorm3 = LayerNormalization(epsilon = layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "            \n",
    "        Returns:\n",
    "            out3 -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            attn_weights_block1 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "            attn_weights_block2 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        # calculating self-attention\n",
    "        mult_attn_out1, attn_weights_block1 = self.mha1(x, \n",
    "                                                        x, \n",
    "                                                        attention_mask=look_ahead_mask, \n",
    "                                                        training=training, \n",
    "                                                        return_attention_scores=True)  # (batch_size, target_seq_len, embedding_dim)\n",
    "        \n",
    "        # apply skip conection and layer normalization \n",
    "        Q1 = self.layernorm1(mult_attn_out1 + x)\n",
    "\n",
    "        # calculatin self-attention using the Q1 and the encoder output \n",
    "        mult_attn_out2, attn_weights_block2 = self.mha2(Q1, \n",
    "                                                        enc_output, \n",
    "                                                        attention_mask=padding_mask, \n",
    "                                                        training=training, \n",
    "                                                        return_attention_scores=True)  # (batch_size, target_seq_len, embedding_dim)\n",
    "        \n",
    "        # applying skip conection and layer normalization \n",
    "        mult_attn_out2 = self.layernorm2(mult_attn_out2 + Q1)  # (batch_size, target_seq_len, embedding_dim)\n",
    "                \n",
    "        # passing the output through a ffn\n",
    "        ffn_output = self.ffn(mult_attn_out2)  # (batch_size, target_seq_len, embedding_dim)\n",
    "        \n",
    "        # applying a dropout to the ffn output during training\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
    "        \n",
    "        # apply skip conection and layer normalization \n",
    "        out3 = self.layernorm3(ffn_output + mult_attn_out2)  # (batch_size, target_seq_len, embedding_dim)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-2'></a> \n",
    "### 5.2 - Full Decoder\n",
    "\n",
    "\n",
    "<center><img src=\"images/decoder.png\" alt=\"Encoder\" width=\"30%\"/></center>\n",
    "<caption><center><b>Figure 3b:</b> Transformer Decoder</center></caption>\n",
    "\n",
    "<a name='ex-7'></a>     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McS3by6k4pnP"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the target input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "        \n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_layers, \n",
    "                 embedding_dim, \n",
    "                 num_heads, \n",
    "                 fully_connected_dim, \n",
    "                 target_vocab_size,\n",
    "                 maximum_position_encoding, \n",
    "                 dropout_rate=0.1, \n",
    "                 layernorm_eps=1e-6):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim = self.embedding_dim,\n",
    "                                        num_heads = num_heads,\n",
    "                                        fully_connected_dim = fully_connected_dim,\n",
    "                                        dropout_rate = dropout_rate,\n",
    "                                        layernorm_eps = layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "            \n",
    "        Returns:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        # creating word embeddings \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, embedding_dim)\n",
    "        \n",
    "        # scaling word embeddings\n",
    "        x = tf.cast(x, tf.float32)*self.embedding_dim**(1/2)\n",
    "        \n",
    "        # calculating positional encodings and adding to word embeddings\n",
    "        x += self.pos_encoding[:,:seq_len,:]\n",
    "\n",
    "        # applying a dropout to x during training\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # stacking multiple decoder layers\n",
    "            x, block1, block2 = self.dec_layers[i](x, \n",
    "                                                   enc_output, \n",
    "                                                   training,\n",
    "                                                   look_ahead_mask, \n",
    "                                                   padding_mask)\n",
    "            \n",
    "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
    "            \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a> \n",
    "## 6 - Transformer \n",
    "\n",
    "<center><img src=\"images/transformer.png\" alt=\"Transformer\" width=\"50%\"/></center>\n",
    "<caption><center><b>Figure 4:</b> Transformer</center></caption>\n",
    "\n",
    "    \n",
    "The flow of data through the Transformer Architecture is as follows:\n",
    "* First your input passes through an Encoder, which is just repeated Encoder layers that you implemented:\n",
    "    - embedding and positional encoding of your input\n",
    "    - multi-head attention on your input\n",
    "    - feed forward neural network to help detect features <br><br>\n",
    "    \n",
    "* Then the predicted output passes through a Decoder, consisting of the decoder layers that you implemented:\n",
    "    - embedding and positional encoding of the output\n",
    "    - multi-head attention on your generated output\n",
    "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
    "    - a feed forward neural network to help detect features <br><br>\n",
    "    \n",
    "* Finally, after the Nth Decoder layer, one dense layer and a softmax are applied to generate prediction for the next output in your sequence.\n",
    "\n",
    "<a name='ex-8'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHymPmaj-2ba"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_layers, \n",
    "                 embedding_dim, \n",
    "                 num_heads, \n",
    "                 fully_connected_dim,\n",
    "                 input_vocab_size,\n",
    "                 target_vocab_size, \n",
    "                 max_positional_encoding_input, \n",
    "                 max_positional_encoding_target, \n",
    "                 dropout_rate=0.1, \n",
    "                 layernorm_eps=1e-6):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers = num_layers,\n",
    "                               embedding_dim = embedding_dim,\n",
    "                               num_heads = num_heads,\n",
    "                               fully_connected_dim = fully_connected_dim,\n",
    "                               input_vocab_size = input_vocab_size,\n",
    "                               maximum_position_encoding = max_positional_encoding_input,\n",
    "                               dropout_rate = dropout_rate,\n",
    "                               layernorm_eps = layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers = num_layers, \n",
    "                               embedding_dim = embedding_dim,\n",
    "                               num_heads = num_heads,\n",
    "                               fully_connected_dim = fully_connected_dim,\n",
    "                               target_vocab_size = target_vocab_size, \n",
    "                               maximum_position_encoding = max_positional_encoding_target,\n",
    "                               dropout_rate = dropout_rate,\n",
    "                               layernorm_eps = layernorm_eps)\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, \n",
    "             input_sentence, \n",
    "             output_sentence, \n",
    "             training, \n",
    "             enc_padding_mask, \n",
    "             look_ahead_mask, \n",
    "             dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            input_sentence -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "                              An array of the indexes of the words in the input sentence\n",
    "            output_sentence -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "                              An array of the indexes of the words in the output sentence\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            enc_padding_mask -- Boolean mask to ensure that the padding is not treated as part of the input\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            dec_padding_mask -- Boolean mask for the second multihead attention layer\n",
    "            \n",
    "        Returns:\n",
    "            final_output -- the full sentence prediction\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # calling the encoder \n",
    "        enc_output = self.encoder(input_sentence, \n",
    "                                  training,\n",
    "                                  enc_padding_mask)  # (batch_size, inp_seq_len, fully_connected_dim)\n",
    "\n",
    "        # calling the decoder \n",
    "        dec_output, attention_weights = self.decoder(output_sentence, \n",
    "                                                     enc_output, \n",
    "                                                     training, \n",
    "                                                     look_ahead_mask, \n",
    "                                                     dec_padding_mask) # (batch_size, tar_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # passing hte decoder output through a linear layer and softmax\n",
    "        final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

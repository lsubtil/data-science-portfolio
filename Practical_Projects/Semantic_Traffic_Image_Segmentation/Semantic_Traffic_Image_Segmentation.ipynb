{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3NVDfXY4__X"
   },
   "source": [
    "# Semantic Traffic Image Segmentation\n",
    "\n",
    "We'll build a semantic traffic image segmentation model, using a UNet network. This kind of task allows you to predict a precise mask for each object in the image by labeling each pixel with its corresponding class. \n",
    "\n",
    "<center><img src=\"images/carseg.png\" width=\"45%\" height=\"45%\"></center>\n",
    "<caption><center><b>Figure 1</b>: Example of a segmented image</center></caption>\n",
    "<br>\n",
    "\n",
    "Region-specific labeling is a pretty crucial consideration for self-driving cars, which require a pixel-perfect understanding of their environment so they can change lanes and avoid other cars, or any number of traffic obstacles that can put peoples' lives in danger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F57bqletV992"
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0exJ9KsDrwck"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Dataset\n",
    "\n",
    "<a name='1.1'></a>\n",
    "### 1.1 - Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWpkuq4tfU4i"
   },
   "outputs": [],
   "source": [
    "path = ''\n",
    "image_path = os.path.join(path, './data/CameraRGB/')\n",
    "mask_path = os.path.join(path, './data/CameraMask/')\n",
    "\n",
    "image_list_orig = os.listdir(image_path)\n",
    "image_list = [image_path + i for i in image_list_orig]\n",
    "mask_list = [mask_path + i for i in image_list_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "RZhnXflBl6Xm",
    "outputId": "dcdd7563-53b9-4ce3-f400-e011f4df9cdc"
   },
   "outputs": [],
   "source": [
    "# Visualize unmasked and masked images from the dataset\n",
    "N = 2\n",
    "img = imageio.imread(image_list[N])\n",
    "mask = imageio.imread(mask_list[N])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 6))\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title('Image')\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "axes[1].imshow(mask[:, :, 0])\n",
    "axes[1].set_title('Segmentation')\n",
    "axes[1].set_xticks([])\n",
    "axes[1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlzMS0mhmkb1",
    "outputId": "e2ad8c66-c380-400f-aed0-9f4b1f53ecad"
   },
   "outputs": [],
   "source": [
    "# Ensure to load the same pattern files (.png)\n",
    "image_list_ds = tf.data.Dataset.list_files(image_list, shuffle=False)\n",
    "mask_list_ds = tf.data.Dataset.list_files(mask_list, shuffle=False)\n",
    "\n",
    "for path in zip(image_list_ds.take(1), mask_list_ds.take(1)):\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNF2Ztii8-Jx",
    "outputId": "7e91a651-a54c-4838-e1db-41ef4915680e"
   },
   "outputs": [],
   "source": [
    "# Create the image path dataset\n",
    "image_filenames = tf.constant(image_list)\n",
    "masks_filenames = tf.constant(mask_list)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_filenames, masks_filenames))\n",
    "\n",
    "for image, mask in dataset.take(1):\n",
    "    print(image)\n",
    "    print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Data treatment\n",
    "\n",
    "We need first to decode png files into 3 channels images, so we can normalize and resize all of them to a standard shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUjQfI1wmkkn"
   },
   "outputs": [],
   "source": [
    "def process_path(image_path, mask_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=3)\n",
    "    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n",
    "    \n",
    "    return img, mask\n",
    "\n",
    "\n",
    "def preprocess(image, mask):\n",
    "    input_image = tf.image.resize(image, (96, 128), method='nearest')\n",
    "    input_mask = tf.image.resize(mask, (96, 128), method='nearest')\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "image_ds = dataset.map(process_path)\n",
    "processed_image_ds = image_ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - UNet Model\n",
    "\n",
    "U-Net uses a matching number of convolutions for downsampling the input image and transposed convolutions for upsampling back up to the original input image size. It also adds skip connections, to retain information that would otherwise become lost during encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - Model architecture\n",
    "\n",
    "<center><img src=\"images/unet.png\" width=\"60%\" height=\"60%\"></center>\n",
    "<caption><center><b>Figure 2</b>: U-Net Architecture</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETPr2Kx7CpqG"
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Downsampling block\n",
    "\n",
    "The contracting path follows a regular CNN architecture, with the repeated application of two 3 x 3 valid padding convolutions, each followed by a rectified linear unit (ReLU) and a 2 x 2 max pooling operation with stride 2 for downsampling. At each downsampling step, the number of feature channels is doubled.\n",
    "<br><br>\n",
    "\n",
    "<center><img src=\"images/encoder.png\" width=\"50%\" height=\"50%\"></center>\n",
    "<caption><center><b>Figure 3</b>: The U-Net Encoder<br></center></caption>\n",
    "<br>\n",
    "\n",
    "The function will return two tensors: \n",
    "- `next_layer`: That will go into the next block. \n",
    "- `skip_connection`: That will go into the corresponding decoding block.\n",
    "\n",
    "Notice that the output used to create the skip connection it's from the Conv2D (or Dropout) layer, and not from MaxPooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jREFwsA5w6j",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bc67a8f4f19dea5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def conv_block(inputs=None, n_filters=64, dropout_prob=0, max_pooling=True):\n",
    "    \"\"\"\n",
    "    Convolutional downsampling block\n",
    "    \n",
    "    Arguments:\n",
    "        inputs -- Input tensor\n",
    "        n_filters -- Number of filters for the convolutional layers\n",
    "        dropout_prob -- Dropout probability\n",
    "        max_pooling -- Use MaxPooling2D to reduce the spatial dimensions of the output volume\n",
    "        \n",
    "    Returns: \n",
    "        next_layer, skip_connection --  Next layer and skip connection outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    conv = Conv2D(n_filters,\n",
    "                  3,   \n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(inputs)\n",
    "    \n",
    "    conv = Conv2D(n_filters,\n",
    "                  3,\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(conv)\n",
    "\n",
    "    if dropout_prob > 0:\n",
    "        conv = Dropout(dropout_prob)(conv)\n",
    "\n",
    "    if max_pooling:\n",
    "        next_layer = MaxPooling2D(strides=(2,2))(conv)\n",
    "        \n",
    "    else:\n",
    "        next_layer = conv\n",
    "        \n",
    "    skip_connection = conv\n",
    "    \n",
    "    return next_layer, skip_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n-9c0keCtbf"
   },
   "source": [
    "<a name='2-3'></a>\n",
    "### 2.3 - Upsampling block\n",
    "\n",
    "The expanding path performs the opposite operation of the contracting path, growing the image back to its original size, while shrinking the channels gradually. In each step, we first concatenate the correspondingly cropped feature map from the contracting path and then upsamples the output through two 3 x 3 convolutions, each followed by a ReLU. At the end, we feed the feature map to a 2 x 2 transposed convolution, which halves the number of feature channels, while growing the height and width of the image. \n",
    "\n",
    "<center><img src=\"images/decoder.png\" width=\"60%\" height=\"60%\"></center>\n",
    "<caption><center><b>Figure 4</b>: The U-Net Decoder</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lzEn-mu6nHa",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a6bea191d41d977",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def upsampling_block(expansive_input, contractive_input, n_filters=64):\n",
    "    \"\"\"\n",
    "    Convolutional upsampling block\n",
    "    \n",
    "    Arguments:\n",
    "        expansive_input -- Input tensor from previous layer\n",
    "        contractive_input -- Input tensor from previous skip layer\n",
    "        n_filters -- Number of filters for the convolutional layers\n",
    "        \n",
    "    Returns: \n",
    "        conv -- Tensor output\n",
    "    \"\"\"\n",
    "\n",
    "    up = Conv2DTranspose(n_filters,    \n",
    "                         3,    \n",
    "                         strides=(2, 2),\n",
    "                         padding='same')(expansive_input)\n",
    "    \n",
    "    merge = concatenate([up, contractive_input], axis=3)\n",
    "    \n",
    "    conv = Conv2D(n_filters,   \n",
    "                  3,     \n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(merge)\n",
    "    \n",
    "    conv = Conv2D(n_filters,  \n",
    "                  3,   \n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(conv)\n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-4'></a>\n",
    "### 2.4 - Building the model\n",
    "\n",
    "In the final layer, a 1x1 convolution is used to map each of the 64 feature vector components to the desired number of classes. In this self-driving car dataset, there are 23 possible labels for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sv2UCFehHZsh",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e43cf8104499fbd9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def unet_model(input_size=(96, 128, 3), n_filters=64, n_classes=23):\n",
    "    \"\"\"\n",
    "    Unet model\n",
    "    \n",
    "    Arguments:\n",
    "        input_size -- Input shape \n",
    "        n_filters -- Number of filters for the convolutional layers\n",
    "        n_classes -- Number of output classes\n",
    "    Returns: \n",
    "        model -- tf.keras.Model\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Contracting Path (encoding)\n",
    "    cblock1 = conv_block(inputs, n_filters)\n",
    "    cblock2 = conv_block(cblock1[0], 2*n_filters)\n",
    "    cblock3 = conv_block(cblock2[0], 4*n_filters)\n",
    "    cblock4 = conv_block(cblock3[0], 8*n_filters, dropout_prob=0.3) \n",
    "    cblock5 = conv_block(cblock4[0], 16*n_filters, dropout_prob=0.3, max_pooling=False) \n",
    "    \n",
    "    # Expanding Path (decoding)\n",
    "    ublock6 = upsampling_block(cblock5[0], cblock4[1], 8*n_filters)\n",
    "    ublock7 = upsampling_block(ublock6, cblock3[1], 4*n_filters)\n",
    "    ublock8 = upsampling_block(ublock7, cblock2[1], 2*n_filters)\n",
    "    ublock9 = upsampling_block(ublock8, cblock1[1], n_filters)\n",
    "\n",
    "    conv9 = Conv2D(n_filters,\n",
    "                   3,\n",
    "                   activation='relu',\n",
    "                   padding='same',\n",
    "                   kernel_initializer='he_normal')(ublock9)\n",
    "    conv10 = Conv2D(n_classes, 1, padding='same')(conv9)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-5'></a>\n",
    "### 2.5 - Model dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCQIwZlnsDTQ"
   },
   "outputs": [],
   "source": [
    "img_height = 96\n",
    "img_width = 128\n",
    "num_channels = 3\n",
    "\n",
    "unet = unet_model((img_height, img_width, num_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model summary\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A02eTdbXDDVv"
   },
   "source": [
    "<a name='2-6'></a>\n",
    "### 2.6 - Loss function\n",
    "\n",
    "In the dataset we're using, the pixel labels (from masked images) are integers (from 0-22) assigned in agreement with the class they represent. This is different from categorical crossentropy, where the labels should be one-hot encoded (just 0s and 1s). So we need to use sparse categorical crossentropy as the loss function, to perform pixel-wise multiclass prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGfA5_7NtH9i"
   },
   "outputs": [],
   "source": [
    "unet.compile(optimizer='adam',\n",
    "            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sco-8XdVC-gN"
   },
   "source": [
    "<a name='2.7'></a>\n",
    "### 2.7 - Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Ne0IowRgcom",
    "outputId": "0e68b994-2a09-4cd8-b0d7-c0d042d81144"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "buffer_size = processed_image_ds.cardinality()\n",
    "\n",
    "train_dataset = processed_image_ds.cache().shuffle(buffer_size).batch(batch_size)\n",
    "print(processed_image_ds.element_spec)\n",
    "\n",
    "model_history = unet.fit(train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.8'></a>\n",
    "### 2.8 - Plot history and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss_acc = pd.DataFrame(history.history)\n",
    "\n",
    "# Plotting loss\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(df_loss_acc['loss'])\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('')\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(df_loss_acc['accuracy'])\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz5Z8XdbC6Hg"
   },
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Dataset handling\n",
    "\n",
    "Function that allows display input image, true mask and predicted mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSuxeWlSgU5f"
   },
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    l = len(display_list)\n",
    "    plt.figure(figsize=(5*l, 5*l))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "cqON4c2UGgC4",
    "outputId": "43f3503e-2b8c-4f42-cda0-76c46ada9c13"
   },
   "outputs": [],
   "source": [
    "for image, mask in processed_image_ds.take(1):\n",
    "    sample_image, sample_mask = image, mask\n",
    "    print(mask.shape)\n",
    "    \n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Create predicted masks \n",
    "\n",
    "Function that return the index with the largest class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvFEnJrHhmJo"
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    \n",
    "    return pred_mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - Show predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BX4uCaP2glMo"
   },
   "outputs": [],
   "source": [
    "def show_predictions(dataset, num):\n",
    "    \"\"\"\n",
    "    Displays the first image of each of the num batches\n",
    "    \"\"\"\n",
    "    \n",
    "    for image, mask in dataset.take(num):\n",
    "        pred_mask = unet.predict(image)\n",
    "        display([image[0], mask[0], create_mask(pred_mask)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "5qODM_hRhfR5",
    "outputId": "78a90e2a-d5aa-4c39-e591-9d78e9526404"
   },
   "outputs": [],
   "source": [
    "show_predictions(train_dataset, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_venv",
   "language": "python",
   "name": "deep_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
